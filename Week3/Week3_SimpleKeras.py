# ---
# jupyter:
#   jupytext:
#     text_representation:
#       extension: .py
#       format_name: light
#       format_version: '1.5'
#       jupytext_version: 1.14.4
#   kernelspec:
#     display_name: Python 3 (ipykernel)
#     language: python
#     name: python3
# ---

# # Introduction to TensorFlow and Keras
#
# In this notebook we define a simple network architechture for taking in 1D input. Here we are going to train a network to mimic the behaviour of an arbitrary function.
#
# Once more this notebook is heavily inspired by https://machine-learning-for-physicists.org and is relased under the same Creative Commons License.
#

# +
import matplotlib.pyplot as plt
import numpy as np

import tensorflow as tf
from tensorflow import keras


import matplotlib.style #Some style nonsense
import matplotlib as mpl #Some more style nonsense


#Set default figure size
#mpl.rcParams['figure.figsize'] = [12.0, 8.0] #Inches... of course it is inches
mpl.rcParams["legend.frameon"] = False
mpl.rcParams['figure.dpi']=200 # dots per inch

print("tf.__version__",tf.__version__)

# -

# ### Define our first two networks
# Below we show two example neural network, the first is defined using the Keras Sequential API (easy) and the second uses the Functional API (more flexibile)
#
# The sequential network will have one input neuron and one output neuron and two hidden layers.
# - First hidden layer has 1 input and 20 neurons with sigmoid activation functions
# - Second hidden layer takes 20 inputs (from the first hidden layer) and has 10 neurons with sigmoid activation functions.
# - Output layer is a single neuron with linear activation function.
#
#
# The functional network will have one input neuron and one output neuron and two hidden layers.
# - First hidden layer has 1 input and 20 neurons with ReLU activation functions
# - Second hidden layer takes 20 inputs (from the first hidden layer) and has 10 neurons with ReLU activation functions.
# - Output layer is a single neuron with linear activation function.

# +
#This first example creates a model using the Sequential API where each layer is defined sequentially
model1 = keras.Sequential([
    keras.layers.Input(shape=(1,)),
    keras.layers.Dense(20,activation="sigmoid"),
    keras.layers.Dense(10,activation='sigmoid'),
    keras.layers.Dense(1,activation='linear')
])

## Altenratively we can use the functional API where one has to code the dependence between layers explictly
# In most situations in this course the Sequential API is easier to use, but the functional API offers much more freedom
input_layer =  keras.layers.Input(shape=(1,))
Layer_1 = keras.layers.Dense(20, activation="relu")(input_layer)
Layer_2 = keras.layers.Dense(10, activation="relu")(Layer_1)
output_layer= keras.layers.Dense(1, activation="linear")(Layer_2)
##Defining the model by specifying the input and output layers
model2 = keras.models.Model(inputs=input_layer, outputs=output_layer)

# -

# Having defined our network the next step is to compile it, specifing which loss function and which optimiser we want to use. Here we will use the 'adam' optimiser which is adaptive and much more perfromant than the standard gradient descent algorithm.

model1.compile(loss='mean_squared_error',optimizer='adam')
model2.compile(loss='mean_squared_error',optimizer='adam')

# ### Now let's look at our network
# There are lots of built in functions to understand the network structure here we will look at a couple.
# ` model.summary()` - This prints a string summary of the network.
#

model1.summary()
model2.summary()


# There are a few things to note here:
# * Each layer has it's own name that is autogenerated (dense_1, dense_2, etc.)
# * The first dimension of the output of each layer is `None` this is because that dimension is variable and corresponds to the batchsize
# * Can you work out why each layer has the number of parameters that it does?
#
# If you have some extra packages installed (pydot + graphviz) you can also get a pretty plot of the network using the `kera.util.plot_model` function

# +
#keras.utils.plot_model(model1,show_shapes=True)
# -

# Now lets define our target function

#A wave packet target function
def target_func(y):
    return( np.sin(y)/(1+y**2) ) # a wave packet...


# Rather than using `model.fit` we will use `model.train_on_batch` to illustrate the steps in running the training. Repeatedly running the cell below will have the same effect as increasing the number of batches used in the training.

# +

nbatch = 2000  #Number of batches
batchsize = 100 #Samples per batch
costs=np.zeros(nbatch) #Output array for the costs
costs2=np.zeros(nbatch) #Output array for the costs

#Loop over the batches, get batchsize samples and train the network
for i in range(nbatch):
    x=np.random.uniform(low=-10.0,high=+10.0,size=[batchsize,1]) #Generate batchsize random numbers between -10 and +10
    y=target_func(x) #Evaluate the function for each random number
    costs[i]=model1.train_on_batch(x,y)  #Train the model on this batch of data
    costs2[i]=model2.train_on_batch(x,y)  #Train the model on this batch of data
                        
# -

#Here we plot the cost vs batch number 
fig, ax = plt.subplots()  #I like to make plots using this silly fig,ax method but plot how you like
step=np.arange(nbatch)  # This function gets a range of numbers from 0 up to nbatch-1
ax.plot(step,costs,linewidth=3,label='Model 1',alpha=0.5)
ax.plot(step,costs2,linewidth=3,label='Model 2',alpha=0.5)
ax.set_xlabel("Batch Number")
ax.set_ylabel("Cost")
ax.legend()
ax.set_yscale('log')

# +
# get the output on a 1D grid of points:
N=400 # number of points
y_in=np.zeros([N,1]) # prepare correct shape for network, here N becomes the batch size
y_in[:,0]=np.linspace(-20.0,20.0,N) # fill with interval
y_out=model1.predict_on_batch(y_in) # apply the network to this set of points!
y_out2=model2.predict_on_batch(y_in) # apply the network to this set of points!

# plot it!
fig, ax = plt.subplots() 
ax.plot(y_in,y_out,label="Model 1")
ax.plot(y_in,y_out2,label="Model 2")
ax.plot(y_in,target_func(y_in),label="Target")
ax.set_xlabel("x")
ax.set_ylabel("y")
ax.set_xlim([-20,20])
ax.axvspan(-20,-10,facecolor='lightgrey', alpha=0.5)
ax.axvspan(+10,+20,facecolor='lightgrey', alpha=0.5)
ax.legend()


fig, ax = plt.subplots() 
ax.plot(y_in,y_out-target_func(y_in),label="Model 1")
ax.plot(y_in,y_out2-target_func(y_in),label="Model 2")
ax.set_xlabel("x")
ax.set_ylabel("Network - Target")
ax.set_title("Residual")
ax.set_xlim([-20,20])
ax.axvspan(-20,-10,facecolor='lightgrey', alpha=0.5)
ax.axvspan(+10,+20,facecolor='lightgrey', alpha=0.5)
ax.legend()
# -

# ## Discussion
#
# The main difference between these two models is the actuvation function. Model 1 uses sigmoid and has a smooth curve. Model 2 uses ReLU and has some sharp features in the network output. For low numbers of training batches it isn't obvious that the performance of the network outside of the region it was trained on (-10,10) is worse, but if you increase the number of batches this will become apparent.
#
# ## Further work
#
# ### Exercise 1: Make a better network?
# Can you improve the network model and make a better fit to the inpout function? You could try changing the number of neurons per layer, the number of layers, or the activation functions for each layer. What metric would you use for determining if the network was better?
#
# ### Exercise 2: Try other functions
# Are some functions easier to model than others? What types of functions are the most difficult to model?

# +

modelBack = keras.models.Model(inputs=input_layer, outputs=Layer_2)
modelBack.summary()
backOut=modelBack.predict_on_batch(y_in)
print("Input shape",np.shape(y_in))
print("backOut shape",np.shape(backOut))

# +

colorList=plt.rcParams['axes.prop_cycle'].by_key()['color']
fig, ax = plt.subplots(4,3,sharex=True) 
# Remove vertical space between Axes
fig.subplots_adjust(hspace=0)
# Remove horizontal space between Axes
fig.subplots_adjust(wspace=0)
for i in range(len(backOut[0])):
    j=i%3
    k=i//3
    ax[k,j].plot(y_in,backOut[:,i],label="Neuron "+str(i),color=colorList[i])
    #ax[k,j].legend()
ax[3,1].plot(y_in,y_out2,label="Model 2",color=colorList[0])
ax[3,1].legend()
ax[3,2].plot(y_in,target_func(y_in),label="Target",color=colorList[1])
ax[3,2].legend()
#ax.set_xlabel("x")
#ax.set_ylabel("y")
#ax.set_xlim([-20,20])
#ax.axvspan(-20,-10,facecolor='lightgrey', alpha=0.5)
#ax.axvspan(+10,+20,facecolor='lightgrey', alpha=0.5)
#ax.legend()
# -


