{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Machine Learning for Physicists\n",
    "## Optional Tasks -- Not for credit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook you will try and train neural networks to replicate images produced from functions. The functions to mimic are:\n",
    "1. A circle \n",
    "2. A circle with a hole \n",
    "3. A circle with three holes \n",
    "\n",
    "The code in this notebook is just a copy and paste of the code from the Week2_NetworkVisualisation notebook. At the bottom of the notebook the three target functions are defined.\n",
    "\n",
    "When doing the exercise think about what defines 'good' performance for a neural network or for your python notebook. In the course some of the metrics we use include\n",
    "- The final cost function (e.g. how well does your network match the target)\n",
    "- The network simplicity (e.g. matching the target as well with a simpler network is better)\n",
    "- The network training efficiency (e.g. getting your network trained using as little computing power as possible)\n",
    "- Notebook readability and code clarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports: only numpy and matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from numpy import array, zeros, exp, random, dot, shape, reshape, meshgrid, linspace\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.dpi']=300 # highres display\n",
    "\n",
    "# for subplots within subplots:\n",
    "from matplotlib import gridspec\n",
    "\n",
    "# for nice inset colorbars: (approach changed from lecture 1 'Visualization' notebook)\n",
    "from mpl_toolkits.axes_grid1.inset_locator import InsetPosition\n",
    "\n",
    "# for updating display \n",
    "# (very simple animation)\n",
    "from IPython.display import clear_output\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backpropagation and training routines\n",
    "\n",
    "# this is basically a merger of the backpropagation\n",
    "# code shown in lecture 2 and some of the \n",
    "# visualization code used in the lecture 1 tutorials!\n",
    "\n",
    "def net_f_df(z,activation):\n",
    "    # return both value f(z) and derivative f'(z)\n",
    "    if activation=='sigmoid':\n",
    "        return([1/(1+np.exp(-z)), \n",
    "                1/((1+np.exp(-z))*(1+np.exp(z))) ])\n",
    "    elif activation=='jump': # cheating a bit here: replacing f'(z)=delta(z) by something smooth\n",
    "        return([np.array(z>0,dtype='float'), \n",
    "                10.0/((1+np.exp(-10*z))*(1+np.exp(10*z))) ] )\n",
    "    elif activation=='linear':\n",
    "        return([z,\n",
    "                1.0])\n",
    "    elif activation=='reLU':\n",
    "        return([(z>0)*z,\n",
    "                (z>0)*1.0\n",
    "               ])\n",
    "\n",
    "def forward_step(y,w,b,activation):\n",
    "    \"\"\"\n",
    "    Go from one layer to the next, given a \n",
    "    weight matrix w (shape [n_neurons_in,n_neurons_out])\n",
    "    a bias vector b (length n_neurons_out)\n",
    "    and the values of input neurons y_in \n",
    "    (shape [batchsize,n_neurons_in])\n",
    "    \n",
    "    returns the values of the output neurons in the next layer \n",
    "    (shape [batchsize, n_neurons_out])\n",
    "    \"\"\"    \n",
    "    # calculate values in next layer, from input y\n",
    "    z=np.dot(y,w)+b # w=weights, b=bias vector for next layer\n",
    "    return(net_f_df(z,activation)) # apply nonlinearity and return result\n",
    "\n",
    "def apply_net(x_in): # one forward pass through the network\n",
    "    global Weights, Biases, NumLayers, Activations\n",
    "    global y_layer, df_layer # for storing y-values and df/dz values\n",
    "    \n",
    "    y=np.copy(x_in) # start with input values\n",
    "    y_layer[0]=np.copy(y)\n",
    "    for j in range(NumLayers): # loop through all layers\n",
    "        # j=0 corresponds to the first layer above the input\n",
    "        y,df=forward_step(y,Weights[j],Biases[j],Activations[j]) # one step\n",
    "        df_layer[j]=np.copy(df) # store f'(z) [needed later in backprop]\n",
    "        y_layer[j+1]=np.copy(y) # store f(z) [also needed in backprop]        \n",
    "    return(y)\n",
    "\n",
    "def apply_net_simple(x_in): # one forward pass through the network\n",
    "    # no storage for backprop (this is used for simple tests)\n",
    "    global Weights, Biases, NumLayers, Activations\n",
    "    \n",
    "    y=x_in # start with input values\n",
    "    for j in range(NumLayers): # loop through all layers\n",
    "        # j=0 corresponds to the first layer above the input\n",
    "        y,df=forward_step(y,Weights[j],Biases[j],Activations[j]) # one step\n",
    "    return(y)\n",
    "\n",
    "def backward_step(delta,w,df): \n",
    "    # delta at layer N, of batchsize x layersize(N))\n",
    "    # w between N-1 and N [layersize(N-1) x layersize(N) matrix]\n",
    "    # df = df/dz at layer N-1, of batchsize x layersize(N-1)\n",
    "    return( np.dot(delta,np.transpose(w))*df )\n",
    "\n",
    "def backprop(y_target): # one backward pass through the network\n",
    "    # the result will be the 'dw_layer' matrices that contain\n",
    "    # the derivatives of the cost function with respect to\n",
    "    # the corresponding weight\n",
    "    global y_layer, df_layer, Weights, Biases, NumLayers\n",
    "    global dw_layer, db_layer # dCost/dw and dCost/db (w,b=weights,biases)\n",
    "\n",
    "    batchsize=np.shape(y_target)[0]\n",
    "    delta=(y_layer[-1]-y_target)*df_layer[-1]\n",
    "    dw_layer[-1]=np.dot(np.transpose(y_layer[-2]),delta)/batchsize\n",
    "    db_layer[-1]=delta.sum(0)/batchsize\n",
    "    for j in range(NumLayers-1):\n",
    "        delta=backward_step(delta,Weights[-1-j],df_layer[-2-j])\n",
    "        dw_layer[-2-j]=np.dot(np.transpose(y_layer[-3-j]),delta)/batchsize # batchsize was missing in old code?\n",
    "        db_layer[-2-j]=delta.sum(0)/batchsize\n",
    "        \n",
    "def gradient_step(eta): # update weights & biases (after backprop!)\n",
    "    global dw_layer, db_layer, Weights, Biases\n",
    "    \n",
    "    for j in range(NumLayers):\n",
    "        Weights[j]-=eta*dw_layer[j]\n",
    "        Biases[j]-=eta*db_layer[j]\n",
    "        \n",
    "def train_net(x_in,y_target,eta): # one full training batch\n",
    "    # x_in is an array of size batchsize x (input-layer-size)\n",
    "    # y_target is an array of size batchsize x (output-layer-size)\n",
    "    # eta is the stepsize for the gradient descent\n",
    "    global y_out_result\n",
    "    \n",
    "    y_out_result=apply_net(x_in)\n",
    "    backprop(y_target)\n",
    "    gradient_step(eta)\n",
    "    cost=0.5*((y_target-y_out_result)**2).sum()/np.shape(x_in)[0]\n",
    "    return(cost)\n",
    "\n",
    "def init_layer_variables(weights,biases,activations):\n",
    "    global Weights, Biases, NumLayers, Activations\n",
    "    global LayerSizes, y_layer, df_layer, dw_layer, db_layer\n",
    "\n",
    "    Weights=weights\n",
    "    Biases=biases\n",
    "    Activations=activations\n",
    "    NumLayers=len(Weights)\n",
    "\n",
    "    LayerSizes=[2]\n",
    "    for j in range(NumLayers):\n",
    "        LayerSizes.append(len(Biases[j]))\n",
    "\n",
    "    y_layer=[[] for j in range(NumLayers+1)]\n",
    "    df_layer=[[] for j in range(NumLayers)]\n",
    "    dw_layer=[np.zeros([LayerSizes[j],LayerSizes[j+1]]) for j in range(NumLayers)]\n",
    "    db_layer=[np.zeros(LayerSizes[j+1]) for j in range(NumLayers)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization routines:\n",
    "\n",
    "# some internal routines for plotting the network:\n",
    "def plot_connection_line(ax,X,Y,W,vmax=1.0,linewidth=3):\n",
    "    t=np.linspace(0,1,20)\n",
    "    if W>0:   #Pick colour of line based on if weight is positive or negative\n",
    "        col=[0,0.4,0.8]  \n",
    "    else:\n",
    "        col=[1,0.3,0]\n",
    "    ax.plot(X[0]+(3*t**2-2*t**3)*(X[1]-X[0]),Y[0]+t*(Y[1]-Y[0]),\n",
    "           alpha=abs(W)/vmax,color=col,\n",
    "           linewidth=linewidth)\n",
    "    \n",
    "def plot_neuron_alpha(ax,X,Y,B,size=100.0,vmax=1.0):\n",
    "    if B>0: #Pick colour of neuron dot based on if bias is positive or negative\n",
    "        col=[0,0.4,0.8]\n",
    "    else:\n",
    "        col=[1,0.3,0]\n",
    "    ax.scatter([X],[Y],marker='o',c=np.atleast_2d([col]),alpha=abs(B)/vmax,s=size,zorder=10)\n",
    "\n",
    "def plot_neuron(ax,X,Y,B,size=100.0,vmax=1.0):\n",
    "    if B>0:\n",
    "        col=[0,0.4,0.8]\n",
    "    else:\n",
    "        col=[1,0.3,0]\n",
    "    ax.scatter([X],[Y],marker='o',c=np.atleast_2d([col]),s=size,zorder=10)\n",
    "    \n",
    "def visualize_network(weights,biases,activations,\n",
    "                      M=100,x0range=[-1,1],x1range=[-1,1],\n",
    "                     size=400.0, linewidth=5.0,\n",
    "                     weights_are_swapped=False,\n",
    "                    layers_already_initialized=False,\n",
    "                      plot_cost_function=None,\n",
    "                      current_cost=None, cost_max=None, plot_target=None\n",
    "                     ):\n",
    "    \"\"\"\n",
    "    Visualize a neural network with 2 input \n",
    "    neurons and 1 output neuron (plot output vs input in a 2D plot)\n",
    "    \n",
    "    weights is a list of the weight matrices for the\n",
    "    layers, where weights[j] is the matrix for the connections\n",
    "    from layer j to layer j+1 (where j==0 is the input)\n",
    "    \n",
    "    weights[j][m,k] is the weight for input neuron k going to output neuron m\n",
    "    (note: internally, m and k are swapped, see the explanation of\n",
    "    batch processing in lecture 2)\n",
    "    \n",
    "    biases[j] is the vector of bias values for obtaining the neurons in layer j+1\n",
    "    biases[j][k] is the bias for neuron k in layer j+1\n",
    "    \n",
    "    activations is a list of the activation functions for\n",
    "    the different layers: choose 'linear','sigmoid',\n",
    "    'jump' (i.e. step-function), and 'reLU'\n",
    "    \n",
    "    M is the resolution (MxM grid)\n",
    "    \n",
    "    x0range is the range of x0 neuron values (horizontal axis)\n",
    "    x1range is the range of x1 neuron values (vertical axis)\n",
    "    \"\"\"\n",
    "    if not weights_are_swapped:\n",
    "        swapped_weights=[]\n",
    "        for j in range(len(weights)):\n",
    "            swapped_weights.append(np.transpose(weights[j]))\n",
    "    else:\n",
    "        swapped_weights=weights\n",
    "\n",
    "    x0,x1=np.meshgrid(np.linspace(x0range[0],x0range[1],M),np.linspace(x1range[0],x1range[1],M))\n",
    "    x_in=np.zeros([M*M,2])\n",
    "    x_in[:,0]=x0.flatten()\n",
    "    x_in[:,1]=x1.flatten()\n",
    "    \n",
    "    # if we call visualization directly, we still\n",
    "    # need to initialize the 'Weights' and other\n",
    "    # global variables; otherwise (during training)\n",
    "    # all of this has already been taken care of:\n",
    "    if not layers_already_initialized:\n",
    "        init_layer_variables(swapped_weights,biases,activations)\n",
    "    y_out=apply_net_simple(x_in)\n",
    "\n",
    "    if plot_cost_function is None:\n",
    "        fig,ax=plt.subplots(ncols=2,nrows=1,figsize=(8,4))\n",
    "    else:\n",
    "        fig=plt.figure(figsize=(8,4))\n",
    "        gs_top = gridspec.GridSpec(nrows=1, ncols=2)\n",
    "        gs_left = gridspec.GridSpecFromSubplotSpec(nrows=2, ncols=1, subplot_spec=gs_top[0], height_ratios=[1.0,0.3])\n",
    "        ax=[ fig.add_subplot(gs_left[0]),\n",
    "            fig.add_subplot(gs_top[1]),\n",
    "           fig.add_subplot(gs_left[1]) ]\n",
    "        # ax[0] is network\n",
    "        # ax[1] is image produced by network\n",
    "        # ax[2] is cost function subplot\n",
    "        \n",
    "    # plot the network itself:\n",
    "    \n",
    "    # positions of neurons on plot:\n",
    "    posX=[[-0.5,+0.5]]; posY=[[0,0]]\n",
    "    vmax=0.0 # for finding the maximum weight\n",
    "    vmaxB=0.0 # for maximum bias\n",
    "    for j in range(len(biases)):\n",
    "        n_neurons=len(biases[j])\n",
    "        posX.append(np.array(range(n_neurons))-0.5*(n_neurons-1))\n",
    "        posY.append(np.full(n_neurons,j+1))\n",
    "        vmax=np.maximum(vmax,np.max(np.abs(weights[j])))\n",
    "        vmaxB=np.maximum(vmaxB,np.max(np.abs(biases[j])))\n",
    "\n",
    "    # plot connections\n",
    "    for j in range(len(biases)):\n",
    "        for k in range(len(posX[j])):\n",
    "            for m in range(len(posX[j+1])):\n",
    "                plot_connection_line(ax[0],[posX[j][k],posX[j+1][m]],\n",
    "                                     [posY[j][k],posY[j+1][m]],\n",
    "                                     swapped_weights[j][k,m],vmax=vmax,\n",
    "                                    linewidth=linewidth)\n",
    "    \n",
    "    # plot neurons\n",
    "    for k in range(len(posX[0])): # input neurons (have no bias!)\n",
    "        plot_neuron(ax[0],posX[0][k],posY[0][k],\n",
    "                   vmaxB,vmax=vmaxB,size=size)\n",
    "    for j in range(len(biases)): # all other neurons\n",
    "        for k in range(len(posX[j+1])):\n",
    "            plot_neuron(ax[0],posX[j+1][k],posY[j+1][k],\n",
    "                       biases[j][k],vmax=vmaxB,size=size)\n",
    "    \n",
    "    ax[0].axis('off')\n",
    "    \n",
    "    # now: the output of the network\n",
    "    img=ax[1].imshow(np.reshape(y_out,[M,M]),origin='lower',\n",
    "                    extent=[x0range[0],x0range[1],x1range[0],x1range[1]])\n",
    "    ax[1].set_xlabel(r'$x_0$')\n",
    "    ax[1].set_ylabel(r'$x_1$')\n",
    "    \n",
    "#     axins1 = inset_axes(ax[1],\n",
    "#                     width=\"40%\",  # width = 50% of parent_bbox width\n",
    "#                     height=\"5%\",  # height : 5%\n",
    "#                     loc='upper right',\n",
    "#                        bbox_to_anchor=[0.3,0.4])\n",
    "\n",
    "#    axins1 = ax[1].inset_axes([0.5,0.8,0.45,0.1])\n",
    "    axins1 = plt.axes([0, 0, 1, 1])\n",
    "    ip = InsetPosition(ax[1], [0.25, 0.1, 0.5, 0.05])\n",
    "    axins1.set_axes_locator(ip)\n",
    "\n",
    "    imgmin=np.min(y_out)\n",
    "    imgmax=np.max(y_out)\n",
    "    color_bar=fig.colorbar(img, cax=axins1, orientation=\"horizontal\",ticks=np.linspace(imgmin,imgmax,3))\n",
    "    cbxtick_obj = plt.getp(color_bar.ax.axes, 'xticklabels')\n",
    "    plt.setp(cbxtick_obj, color=\"white\")\n",
    "    axins1.xaxis.set_ticks_position(\"bottom\")\n",
    "\n",
    "    if plot_target is not None:\n",
    "        axins2 = plt.axes([0.01, 0.01, 0.99, 0.99])\n",
    "        ip = InsetPosition(ax[1], [0.75, 0.75, 0.2, 0.2])\n",
    "        axins2.set_axes_locator(ip)\n",
    "        axins2.imshow(plot_target,origin='lower')\n",
    "        axins2.get_xaxis().set_ticks([])\n",
    "        axins2.get_yaxis().set_ticks([])\n",
    "        \n",
    "    if plot_cost_function is not None:\n",
    "        ax[2].plot(plot_cost_function)\n",
    "        ax[2].set_ylim([0.0,cost_max])\n",
    "        ax[2].set_yticks([0.0,cost_max])\n",
    "        ax[2].set_yticklabels([\"0\",'{:1.2e}'.format(cost_max)])\n",
    "        if current_cost is not None:\n",
    "            ax[2].text(0.9, 0.9, 'cost={:1.2e}'.format(current_cost), horizontalalignment='right',\n",
    "                       verticalalignment='top', transform=ax[2].transAxes)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "def visualize_network_training(weights,biases,activations,\n",
    "                               target_function,\n",
    "                               num_neurons=None,\n",
    "                               weight_scale=1.0,\n",
    "                               bias_scale=1.0,\n",
    "                               xspread=1.0,\n",
    "                      M=100,x0range=[-1,1],x1range=[-1,1],\n",
    "                     size=400.0, linewidth=5.0,\n",
    "                    steps=100, batchsize=10, eta=0.1,\n",
    "                              random_init=False,\n",
    "                              visualize_nsteps=1,\n",
    "                              plot_target=True):\n",
    "    \"\"\"\n",
    "    Visualize the training of a neural network.\n",
    "    \n",
    "    weights, biases, and activations define the neural network \n",
    "    (the starting point of the optimization; for the detailed description,\n",
    "    see the help for visualize_network)\n",
    "    \n",
    "    If you want to have layers randomly initialized, just provide\n",
    "    the number of neurons for each layer as 'num_neurons'. This should include\n",
    "    all layers, including input (2 neurons) and output (1), so num_neurons=[2,3,5,4,1] is\n",
    "    a valid example. In this case, weight_scale and bias_scale define the\n",
    "    spread of the random Gaussian variables used to initialize all weights and biases.\n",
    "    \n",
    "    target_function is the name of the function that we\n",
    "    want to approximate; it must be possible to \n",
    "    evaluate this function on a batch of samples, by\n",
    "    calling target_function(y) on an array y of \n",
    "    shape [batchsize,2], where\n",
    "    the second index refers to the two coordinates\n",
    "    (input neuron values) x0 and x1. The return\n",
    "    value must be an array with one index, corresponding\n",
    "    to the batchsize. A valid example is:\n",
    "    \n",
    "    def my_target(y):\n",
    "        return( np.sin(y[:,0]) + np.cos(y[:,1]) )\n",
    "    \n",
    "    steps is the number of training steps\n",
    "    \n",
    "    batchsize is the number of samples per training step\n",
    "    \n",
    "    eta is the learning rate (stepsize in the gradient descent)\n",
    "    \n",
    "    xspread denotes the spread of the Gaussian\n",
    "    used to sample points in (x0,x1)-space\n",
    "    \n",
    "    visualize_n_steps>1 means skip some steps before\n",
    "    visualizing again (can speed up things)\n",
    "    \n",
    "    plot_target=True means do plot the target function in a corner\n",
    "    \n",
    "    For all the other parameters, see the help for\n",
    "        visualize_network\n",
    "    \n",
    "    weights and biases as given here will be used\n",
    "    as starting points, unless you specify\n",
    "    random_init=True, in which case they will be\n",
    "    used to determine the spread of Gaussian random\n",
    "    variables used for initialization!\n",
    "    \"\"\"\n",
    "    \n",
    "    if num_neurons is not None: # build weight matrices as randomly initialized\n",
    "        weights=[weight_scale*np.random.randn(num_neurons[j+1],num_neurons[j]) for j in range(len(num_neurons)-1)]\n",
    "        biases=[bias_scale*np.random.randn(num_neurons[j+1]) for j in range(len(num_neurons)-1)]\n",
    "    \n",
    "    swapped_weights=[]\n",
    "    for j in range(len(weights)):\n",
    "        swapped_weights.append(np.transpose(weights[j]))\n",
    "    init_layer_variables(swapped_weights,biases,activations)\n",
    "    \n",
    "    if plot_target:\n",
    "        x0,x1=np.meshgrid(np.linspace(x0range[0],x0range[1],M),np.linspace(x1range[0],x1range[1],M))\n",
    "        y=np.zeros([M*M,2])\n",
    "        y[:,0]=x0.flatten()\n",
    "        y[:,1]=x1.flatten()\n",
    "        plot_target_values=np.reshape(target_function(y),[M,M])\n",
    "    else:\n",
    "        plot_target_values=None\n",
    "    \n",
    "    y_target=np.zeros([batchsize,1])\n",
    "    costs=np.zeros(steps)\n",
    "    \n",
    "    for j in range(steps):\n",
    "        # produce samples (random points in x0,x1-space):\n",
    "        x_in=xspread*np.random.randn(batchsize,2)\n",
    "        # apply target function to those points:\n",
    "        y_target[:,0]=target_function(x_in)\n",
    "        # do one training step on this batch of samples:\n",
    "        costs[j]=train_net(x_in,y_target,eta)\n",
    "        \n",
    "        # now visualize the updated network:\n",
    "        if j%visualize_nsteps==0:\n",
    "            clear_output(wait=True) # for animation\n",
    "            if j>10:\n",
    "                cost_max=np.average(costs[0:j])*1.5\n",
    "            else:\n",
    "                cost_max=costs[0]\n",
    "            visualize_network(Weights,Biases,activations,\n",
    "                          M,x0range=x0range,x1range=x1range,\n",
    "                         size=size, linewidth=linewidth,\n",
    "                             weights_are_swapped=True,\n",
    "                             layers_already_initialized=True,\n",
    "                             plot_cost_function=costs,\n",
    "                             current_cost=costs[j],\n",
    "                             cost_max=cost_max,\n",
    "                             plot_target=plot_target_values)\n",
    "            sleep(0.1) # wait a bit before next step (probably not needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Circle target  [40 points]\n",
    "Train a network to mimic the following target (e.g. using the visualize_network_training function above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def circle_target(x):  #A circle of radius 2 around the origin\n",
    "    R=2.0\n",
    "    return( 1.0*( x[:,0]**2+x[:,1]**2<R**2 ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Circle hole target [40 points]\n",
    "Train a network to mimic the following target (e.g. using the visualize_network_training function above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def circle_hole_target(x):#A circle of radius 2, with a hole of radius 1 around the origin\n",
    "    r=1.0; R=2.0\n",
    "    return( 1.0*( x[:,0]**2+x[:,1]**2<R**2 ) - 1.0*( (x[:,0])**2+(x[:,1])**2<r**2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scream target [20 points]\n",
    "Train a network to mimic the following target (e.g. using the visualize_network_training function above). Note this target is much more difficult than the previous two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scream_target(x): #A circle of radius 2, two eyes of radius 0.5 and a mouth of radius 0.8\n",
    "    a=0.8; r=0.5; R=2.0; r2=0.8\n",
    "    return( 1.0*( x[:,0]**2+x[:,1]**2<R**2 ) - 1.0*( (x[:,0]-a)**2+(x[:,1]-a)**2<r**2) - 1.0*( (x[:,0]+a)**2+(x[:,1]-a)**2<r**2 ) - 1.0*( (x[:,0])**2+(x[:,1]+a)**2<r2**2 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
